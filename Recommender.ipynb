{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPEWb/Xa+z98Yr3jcrDtMuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argalusmp/CH2-PS_Recommendation-System/blob/V/Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Recommendation System with Content-Based Filtering**"
      ],
      "metadata": {
        "id": "l8nhDATdPwXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages\n",
        "\n",
        "Import Packages\n",
        "\n"
      ],
      "metadata": {
        "id": "a5u2dta3QRWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "gqGtDqpdQgiz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset\n"
      ],
      "metadata": {
        "id": "-gw06VeaRCOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_dataset = pd.read_csv(\"./user_data.csv\")\n",
        "event_dataset= pd.read_csv(\"./event_data.csv\")"
      ],
      "metadata": {
        "id": "XUo3-g3uRFqz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(user_dataset))\n",
        "print(len(event_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPlV_6I2TdkE",
        "outputId": "d20330ef-bdb7-4b84-f425-c3334fd7e219"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1091\n",
            "1092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data\n"
      ],
      "metadata": {
        "id": "Z7KBW-TR_mX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.merge(user_dataset, event_dataset, how='cross')"
      ],
      "metadata": {
        "id": "rnNdQnXePd4V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan data menjadi train dan test\n",
        "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Hv5LGlTTPpKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing user data\n",
        "user_dataset['Skills'] = user_dataset['Skills'].str.lower()\n",
        "user_dataset['Availability'] = user_dataset['Availability'].str.lower()\n",
        "user_dataset['Location'] = user_dataset['Location'].str.lower()\n",
        "user_dataset['Type of Organization'] = user_dataset['Type of Organization'].str.lower()\n",
        "\n",
        "## Preprocessing event data\n",
        "event_dataset['Kualifikasi'] = event_dataset['Kualifikasi'].str.lower()\n",
        "event_dataset['Domisili'] = event_dataset['Domisili'].str.lower()\n",
        "event_dataset['Kategori'] = event_dataset['Kategori'].str.lower()\n",
        "event_dataset['Age'] = event_dataset['Age'].str.lower()\n",
        "\n",
        "\n",
        "## Memisahkan user skill menjadi beberapa kolom terpisah untuk one hot\n",
        "user_skills_split = user_dataset['Skills'].str.split(', ', expand=True)\n",
        "\n",
        "## Create one-hot encoding for user skills\n",
        "user_skills_one_hot = pd.get_dummies(user_skills_split, prefix='Skill')"
      ],
      "metadata": {
        "id": "w8v2Jmrc_kVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Menggabungkan dataset\n",
        "merged_data = pd.merge(user_dataset, event_dataset, how='cross')\n",
        "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
        "\n",
        "## Create one-hot encoding for user and event data\n",
        "user_one_hot = pd.get_dummies(merged_data[['Age_x', 'Availability', 'Location', 'Type of Organization']], prefix='User')\n",
        "event_one_hot = pd.get_dummies(merged_data[[ 'Kategori', 'Age_y','Domisili']], prefix='Event')"
      ],
      "metadata": {
        "id": "1TS5IErcJUKv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check Display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "event_one_hot\n",
        "#user_one_hot"
      ],
      "metadata": {
        "id": "FH_nBWffvpu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Merge Onehot encoding with data user\n",
        "user_data_encode = pd.concat([user_one_hot, user_skills_one_hot], axis=1)\n",
        "user_data_encode"
      ],
      "metadata": {
        "id": "XC8QVVpmc39b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Memisahkan event kualifikasi menjadi beberapa kolom terpisah untuk one hot\n",
        "event_kualifikasi_split = event_dataset['Kualifikasi'].str.split(', ', expand=True)\n",
        "\n",
        "## Create one-hot encoding for kualifikasi\n",
        "event_kualifikasi_one_hot = pd.get_dummies(event_kualifikasi_split, prefix='Kualifikasi')\n",
        "\n",
        "## Merge Kualifikasi with dataset event after one-hot kualifikasi\n",
        "event_data_encode = pd.concat([event_one_hot, event_kualifikasi_one_hot], axis=1)\n"
      ],
      "metadata": {
        "id": "Ckh299YzcEtD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check event encode display\n",
        "event_data_encode"
      ],
      "metadata": {
        "id": "WBS3Q1O7xzlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_data_encode.isnull().sum())\n",
        "print(event_data_encode.isnull().sum())\n"
      ],
      "metadata": {
        "id": "11LOccTgMewM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For set Y to target\n",
        "#target_columns = ['Kualifikasi_kualifikasi1', 'Kualifikasi_kualifikasi2', ...]\n",
        "#X_train, X_test, y_train, y_test = train_test_split(user_data_encode, event_data_encode[target_columns], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "## Train-test split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HzqxGqliWLJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nyoba proses nilai umur\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "###Pemrosesan Data Umur\n",
        "def process_age(value):\n",
        "    if '-' in str(value):  # Jika nilai adalah rentang umur\n",
        "        age_range = value.split('-')\n",
        "        return (int(age_range[0]) + int(age_range[1])) / 2\n",
        "    elif isinstance(value, int):  # Jika nilai adalah umur tunggal dan sudah integer\n",
        "        return value\n",
        "    else:\n",
        "        # Penanganan lainnya\n",
        "        return None\n",
        "\n",
        "\n",
        "event_dataset['Age'] = event_dataset['Age'].apply(process_age)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "y8lnGwUC_71g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try and Try and Try"
      ],
      "metadata": {
        "id": "PYnicmNlW8VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pusing NaN"
      ],
      "metadata": {
        "id": "nEbsj81dU3a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Baca dataset\n",
        "user_dataset = pd.read_csv(\"./user_data.csv\")\n",
        "event_dataset = pd.read_csv(\"./event_data.csv\")\n",
        "\n",
        "# Preprocessing user data\n",
        "user_dataset['Skills'] = user_dataset['Skills'].str.lower()\n",
        "user_dataset['Availability'] = user_dataset['Availability'].str.lower()\n",
        "user_dataset['Location'] = user_dataset['Location'].str.lower()\n",
        "user_dataset['Type of Organization'] = user_dataset['Type of Organization'].str.lower()\n",
        "\n",
        "# Preprocessing event data\n",
        "event_dataset['Kualifikasi'] = event_dataset['Kualifikasi'].str.lower()\n",
        "event_dataset['Domisili'] = event_dataset['Domisili'].str.lower()\n",
        "event_dataset['Kategori'] = event_dataset['Kategori'].str.lower()\n",
        "event_dataset['Age'] = event_dataset['Age'].astype(str).str.lower()\n",
        "\n",
        "# Gabungkan data\n",
        "full_data = pd.merge(user_dataset, event_dataset, how='cross')\n",
        "\n",
        "# Pisahkan data menjadi train dan test\n",
        "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encoding dan penggabungan data\n",
        "user_skills_split = user_dataset['Skills'].str.split(', ', expand=True)\n",
        "user_skills_one_hot = pd.get_dummies(user_skills_split, prefix='Skill')\n",
        "\n",
        "user_one_hot_train = pd.get_dummies(train_data[['Age_x', 'Availability', 'Location', 'Type of Organization']], prefix='User')\n",
        "event_one_hot_train = pd.get_dummies(train_data[['Kategori', 'Age_y', 'Domisili']], prefix='Event')\n",
        "event_kualifikasi_one_hot_train = pd.get_dummies(train_data['Kualifikasi'].str.split(', ', expand=True), prefix='Kualifikasi')\n",
        "\n",
        "train_data_encode = pd.concat([user_one_hot_train, user_skills_one_hot, event_one_hot_train, event_kualifikasi_one_hot_train], axis=1)\n",
        "\n",
        "# Normalisasi data\n",
        "scaler = StandardScaler()\n",
        "train_data_normalize = scaler.fit_transform(train_data_encode)"
      ],
      "metadata": {
        "id": "WXAFNyBjU25g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_normalize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40FkFztDWq7n",
        "outputId": "e5cc2c79-388d-4494-cb3c-39b8adc2cf34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.48445201, -0.27727253, -0.56223711, ..., -0.21217941,\n",
              "        -0.23023335, -0.25379785],\n",
              "       [-0.88258847, -0.27727253, -0.56223711, ..., -0.21217941,\n",
              "        -0.23023335, -0.25379785],\n",
              "       [ 0.47000609, -0.27727253,  1.77860904, ..., -0.21217941,\n",
              "        -0.23023335, -0.25379785],\n",
              "       ...,\n",
              "       [        nan,         nan,         nan, ...,         nan,\n",
              "                nan,         nan],\n",
              "       [        nan,         nan,         nan, ...,         nan,\n",
              "                nan,         nan],\n",
              "       [        nan,         nan,         nan, ...,         nan,\n",
              "                nan,         nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_normalize = np.nan_to_num(train_data_normalize, nan=np.nanmean(train_data_normalize, axis=0))\n"
      ],
      "metadata": {
        "id": "OCD999T_WEyd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.isnan(train_data_normalize).any())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4H06KBWNn4",
        "outputId": "793a47bb-0821-4018-d651-6964acb0d6f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung similarity matrix menggunakan cosine similarity\n",
        "similarity_matrix = cosine_similarity(train_data_normalize, train_data_normalize)"
      ],
      "metadata": {
        "id": "rG_jvJo3VTF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine Similarity: {similarity_matrix[0][0]}\")"
      ],
      "metadata": {
        "id": "e-V7Xo8FWZsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proses training model\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Sesuaikan dengan layer-layer yang sesuai\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(train_data_normalize.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(train_data_normalize.shape[1], activation='linear')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit model dengan similarity matrix sebagai weights\n",
        "model.fit(train_data_normalize, similarity_matrix, epochs=10, batch_size=32)\n",
        "\n",
        "# Lakukan evaluasi pada data test\n",
        "test_data_encode = pd.concat([pd.get_dummies(test_data[['Age_x', 'Availability', 'Location', 'Type of Organization']], prefix='User'),\n",
        "                              pd.get_dummies(test_data['Skills'].str.split(', ', expand=True), prefix='Skill'),\n",
        "                              pd.get_dummies(test_data[['Kategori', 'Age_y', 'Domisili']], prefix='Event'),\n",
        "                              pd.get_dummies(test_data['Kualifikasi'].str.split(', ', expand=True), prefix='Kualifikasi')], axis=1)\n",
        "\n",
        "test_data_normalize = scaler.transform(test_data_encode)\n",
        "test_similarity_matrix = cosine_similarity(test_data_normalize, train_data_normalize)\n",
        "\n",
        "# Lakukan prediksi\n",
        "predictions = model.predict(test_data_normalize)\n",
        "\n",
        "# Evaluasi model, misalnya menggunakan Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(test_similarity_matrix, predictions)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Dapatkan rekomendasi untuk pengguna tertentu\n",
        "user_index = 0  # Ganti dengan indeks pengguna yang ingin direkomendasikan\n",
        "user_data_normalized = scaler.transform(user_data_encode.iloc[[user_index]])\n",
        "user_similarity = cosine_similarity(user_data_normalized, train_data_normalize)\n",
        "\n",
        "# Hitung bobot rekomendasi\n",
        "weighted_recommendations = np.dot(user_similarity, train_data_normalize)\n",
        "\n",
        "# Ambil indeks item-event yang belum diikuti oleh pengguna\n",
        "not_followed_events = ~train_data['User_ID'].iloc[user_index].isin(test_data['User_ID'])\n",
        "recommendations = weighted_recommendations[:, not_followed_events]\n",
        "\n",
        "# Dapatkan indeks event teratas sebagai rekomendasi\n",
        "top_recommendations = np.argsort(recommendations[0])[::-1][:5]\n",
        "\n",
        "# Cetak rekomendasi\n",
        "print(f'Rekomendasi untuk User {user_index + 1}: {top_recommendations + 1}')\n"
      ],
      "metadata": {
        "id": "Iy0oFeqTVAgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}