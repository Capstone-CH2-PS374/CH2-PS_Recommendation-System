{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argalusmp/CH2-PS_Recommendation-System/blob/V/Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY4aTHkiG5bL"
      },
      "source": [
        "[Collab](https://colab.research.google.com/drive/1d9l2-NXW5traKPQ0j-l4eZ2vSI0mEVvV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8nhDATdPwXe"
      },
      "source": [
        "# **Build Recommendation System with Content-Based Filtering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5u2dta3QRWs"
      },
      "source": [
        "# Packages\n",
        "\n",
        "Import Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "\n",
        "# Load Event dataset\n",
        "event_data = pd.read_csv(\"./events_dataset.csv\")\n",
        "\n",
        "event_df = pd.DataFrame(event_data)\n",
        "\n",
        "# Load User dataset\n",
        "user_data = pd.read_csv(\"./users_dataset.csv\")\n",
        "\n",
        "user_df = pd.DataFrame(user_data)\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_event_data(event_df):\n",
        "     # One-hot encode categorical variables\n",
        "    event_df = pd.get_dummies(event_df, columns=['Category', 'Location'])\n",
        "\n",
        "    # Split Qualifications into separate skills\n",
        "    event_df['Skills'] = event_df['Qualifications'].apply(lambda x: ' '.join(x.lower().split(',')) if pd.notnull(x) else '')\n",
        "    return event_df[list(event_df.columns[3:])]\n",
        "\n",
        "def preprocess_user_data(user_df):\n",
        "    # Convert categorical features to numerical representation\n",
        "\n",
        "    # Split Skills into separate skills\n",
        "    user_df['Skills'] = user_df['Skills'].apply(lambda x: ' '.join(x.lower().split(',')) if pd.notnull(x) else '')\n",
        "\n",
        "    return user_df[['Volunteer Name',  'Gender', 'Skills', 'Location', 'Type of Organization']]\n",
        "\n",
        "event_df = preprocess_event_data(event_df)\n",
        "user_df = preprocess_user_data(user_df)\n",
        "\n",
        "# Create a mapping for skills\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(event_df['Skills'].explode().unique())\n",
        "\n",
        "# Transform event and user skills into binary vectors\n",
        "event_skills = pd.DataFrame(mlb.transform(event_df['Skills']), columns=mlb.classes_)\n",
        "user_skills = pd.DataFrame(mlb.transform(user_df['Skills']), columns=mlb.classes_)\n",
        "\n",
        "# Combine the binary vectors with the original dataframes\n",
        "event_df = pd.concat([event_df, event_skills], axis=1)\n",
        "user_df = pd.concat([user_df, user_skills], axis=1)\n",
        "\n",
        "# Drop the original 'Skills' column\n",
        "event_df.drop('Skills', axis=1, inplace=True)\n",
        "user_df.drop('Skills', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "_fPbRQSskA62",
        "outputId": "9056c5ae-cabd-4de4-f3ac-cdd253feceae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Qualifications'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f2ac11d2ede0>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Create a mapping for skills\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Qualifications'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Transform event and user skills into binary vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Qualifications'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the recommendation model\n",
        "def build_model():\n",
        "    # Input layers\n",
        "    event_input = Input(shape=(event_df.shape[1]-1,), name='event_input')\n",
        "    user_input = Input(shape=(user_df.shape[1]-1,), name='user_input')\n",
        "\n",
        "    # Embedding layers for event and user\n",
        "    event_embedding = Embedding(input_dim=2, output_dim=5, input_length=event_df.shape[1]-1)(event_input)\n",
        "    user_embedding = Embedding(input_dim=2, output_dim=5, input_length=user_df.shape[1]-1)(user_input)\n",
        "\n",
        "    # Flatten the embeddings\n",
        "    event_flatten = Flatten()(event_embedding)\n",
        "    user_flatten = Flatten()(user_embedding)\n",
        "\n",
        "    # Concatenate the flattened embeddings\n",
        "    concat = Concatenate()([event_flatten, user_flatten])\n",
        "\n",
        "    # Dense layers for the recommendation model\n",
        "    dense1 = Dense(128, activation='relu')(concat)\n",
        "    dense2 = Dense(64, activation='relu')(dense1)\n",
        "    output = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=[event_input, user_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4S5iV-vmClv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_event_train, X_event_test, X_user_train, X_user_test, y_train, y_test = train_test_split(\n",
        "    event_df.drop('Event_id', axis=1).values,\n",
        "    user_df.drop('Volunteer Name', axis=1).values,\n",
        "    np.ones(event_df.shape[0]), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert data to NumPy arrays with appropriate data types\n",
        "X_event_train = np.asarray(X_event_train).astype(np.float32)\n",
        "X_event_test = np.asarray(X_event_test).astype(np.float32)\n",
        "X_user_train = np.asarray(X_user_train).astype(np.float32)\n",
        "X_user_test = np.asarray(X_user_test).astype(np.float32)\n",
        "y_train = np.asarray(y_train).astype(np.float32)\n",
        "y_test = np.asarray(y_test).astype(np.float32)\n",
        "\n",
        "# Build and train the model\n",
        "model = build_model()\n",
        "model.fit(x=[X_event_train, X_user_train], y=y_train, epochs=10, batch_size=32, validation_data=([X_event_test, X_user_test], y_test))\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict([event_df.drop('Event_id', axis=1).values, user_df.drop('Volunteer Name', axis=1).values])\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "fYORV-PHl-KJ",
        "outputId": "740bffba-7fdd-438d-fa52-68fc8c2e1338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6600c14077e3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m X_event_train, X_event_test, X_user_train, X_user_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mevent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0muser_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Volunteer Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5397\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5398\u001b[0m         \"\"\"\n\u001b[0;32m-> 5399\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5400\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5401\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4505\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4546\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4547\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6935\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Event_id'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqGtDqpdQgiz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gw06VeaRCOD"
      },
      "source": [
        "# Import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUo3-g3uRFqz"
      },
      "outputs": [],
      "source": [
        "user_dataset = pd.read_csv(\"./users_dataset.csv\")\n",
        "event_dataset= pd.read_csv(\"./events_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro1ezA7xF-c3"
      },
      "outputs": [],
      "source": [
        "df_user = pd.DataFrame(user_dataset)\n",
        "df_event = pd.DataFrame(event_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPlV_6I2TdkE",
        "outputId": "3c3d5242-2592-4dfa-b989-449b7bfa821c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2786\n",
            "2786\n"
          ]
        }
      ],
      "source": [
        "print(len(df_user))\n",
        "print(len(df_event))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbCb8aclGZC8"
      },
      "outputs": [],
      "source": [
        "skills_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "skills_encoded = skills_encoder.fit_transform(df_user[['Skills']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NybQAYiqJWK9"
      },
      "outputs": [],
      "source": [
        "location_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "location_encoded = location_encoder.fit_transform(df_event[['Location']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eAYvs3MJXLS"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding untuk kategori acara\n",
        "category_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "category_encoded = category_encoder.fit_transform(df_event[['Category']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(category_encoded)"
      ],
      "metadata": {
        "id": "rRYk6CclhJDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(event_matrix)"
      ],
      "metadata": {
        "id": "nWcKPZPfhtA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTkyHVPuJYEk"
      },
      "outputs": [],
      "source": [
        "# Bagi data menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_volunteer[['Skills', 'Location', 'Age']], df_volunteer['Target_Label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7KBW-TR_mX0"
      },
      "source": [
        "# Preprocessing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnNdQnXePd4V"
      },
      "outputs": [],
      "source": [
        "all_data = pd.merge(user_dataset, event_dataset, how='cross')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv5LGlTTPpKd"
      },
      "outputs": [],
      "source": [
        "# Pisahkan data menjadi train dan test\n",
        "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8v2Jmrc_kVy"
      },
      "outputs": [],
      "source": [
        "# Preprocessing user data\n",
        "user_dataset['Skills'] = user_dataset['Skills'].str.lower()\n",
        "user_dataset['Availability'] = user_dataset['Availability'].str.lower()\n",
        "user_dataset['Location'] = user_dataset['Location'].str.lower()\n",
        "user_dataset['Type of Organization'] = user_dataset['Type of Organization'].str.lower()\n",
        "\n",
        "## Preprocessing event data\n",
        "event_dataset['Kualifikasi'] = event_dataset['Kualifikasi'].str.lower()\n",
        "event_dataset['Domisili'] = event_dataset['Domisili'].str.lower()\n",
        "event_dataset['Kategori'] = event_dataset['Kategori'].str.lower()\n",
        "event_dataset['Age'] = event_dataset['Age'].str.lower()\n",
        "\n",
        "\n",
        "## Memisahkan user skill menjadi beberapa kolom terpisah untuk one hot\n",
        "user_skills_split = user_dataset['Skills'].str.split(', ', expand=True)\n",
        "\n",
        "## Create one-hot encoding for user skills\n",
        "user_skills_one_hot = pd.get_dummies(user_skills_split, prefix='Skill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TS5IErcJUKv"
      },
      "outputs": [],
      "source": [
        "## Menggabungkan dataset\n",
        "merged_data = pd.merge(user_dataset, event_dataset, how='cross')\n",
        "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
        "\n",
        "## Create one-hot encoding for user and event data\n",
        "user_one_hot = pd.get_dummies(merged_data[['Age_x', 'Availability', 'Location', 'Type of Organization']], prefix='User')\n",
        "event_one_hot = pd.get_dummies(merged_data[[ 'Kategori', 'Age_y','Domisili']], prefix='Event')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH_nBWffvpu1"
      },
      "outputs": [],
      "source": [
        "## Check Display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "event_one_hot\n",
        "#user_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC8QVVpmc39b"
      },
      "outputs": [],
      "source": [
        "## Merge Onehot encoding with data user\n",
        "user_data_encode = pd.concat([user_one_hot, user_skills_one_hot], axis=1)\n",
        "user_data_encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckh299YzcEtD"
      },
      "outputs": [],
      "source": [
        "## Memisahkan event kualifikasi menjadi beberapa kolom terpisah untuk one hot\n",
        "event_kualifikasi_split = event_dataset['Kualifikasi'].str.split(', ', expand=True)\n",
        "\n",
        "## Create one-hot encoding for kualifikasi\n",
        "event_kualifikasi_one_hot = pd.get_dummies(event_kualifikasi_split, prefix='Kualifikasi')\n",
        "\n",
        "## Merge Kualifikasi with dataset event after one-hot kualifikasi\n",
        "event_data_encode = pd.concat([event_one_hot, event_kualifikasi_one_hot], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBS3Q1O7xzlM"
      },
      "outputs": [],
      "source": [
        "## Check event encode display\n",
        "event_data_encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11LOccTgMewM"
      },
      "outputs": [],
      "source": [
        "print(user_data_encode.isnull().sum())\n",
        "print(event_data_encode.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzqxGqliWLJm"
      },
      "outputs": [],
      "source": [
        "## For set Y to target\n",
        "#target_columns = ['Kualifikasi_kualifikasi1', 'Kualifikasi_kualifikasi2', ...]\n",
        "#X_train, X_test, y_train, y_test = train_test_split(user_data_encode, event_data_encode[target_columns], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "## Train-test split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8lnGwUC_71g"
      },
      "source": [
        "Nyoba proses nilai umur\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "###Pemrosesan Data Umur\n",
        "def process_age(value):\n",
        "    if '-' in str(value):  # Jika nilai adalah rentang umur\n",
        "        age_range = value.split('-')\n",
        "        return (int(age_range[0]) + int(age_range[1])) / 2\n",
        "    elif isinstance(value, int):  # Jika nilai adalah umur tunggal dan sudah integer\n",
        "        return value\n",
        "    else:\n",
        "        # Penanganan lainnya\n",
        "        return None\n",
        "\n",
        "\n",
        "event_dataset['Age'] = event_dataset['Age'].apply(process_age)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYnicmNlW8VO"
      },
      "source": [
        "# Try and Try and Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEbsj81dU3a9"
      },
      "source": [
        "# Pusing NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAFNyBjU25g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "3c91f7c5-9fdb-4a4d-d5be-3670dba66c37"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e9001862c8d3>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0muser_skills_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_skills_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Skill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0muser_one_hot_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'Skills'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Type of Organization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'User'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mevent_one_hot_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Qualifications'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Event'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mevent_kualifikasi_one_hot_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Qualifications'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Qualifications'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6135\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Location'] not in index\""
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Baca dataset\n",
        "user_dataset = pd.read_csv(\"./users_dataset.csv\")\n",
        "event_dataset = pd.read_csv(\"./events_dataset.csv\")\n",
        "\n",
        "# Preprocessing user data\n",
        "user_dataset['Skills'] = user_dataset['Skills'].str.lower()\n",
        "user_dataset['Location'] = user_dataset['Location'].str.lower()\n",
        "user_dataset['Type of Organization'] = user_dataset['Type of Organization'].str.lower()\n",
        "\n",
        "# Preprocessing event data\n",
        "event_dataset['Qualifications'] = event_dataset['Qualifications'].str.lower()\n",
        "event_dataset['Location'] = event_dataset['Location'].str.lower()\n",
        "event_dataset['Category'] = event_dataset['Category'].str.lower()\n",
        "\n",
        "# Gabungkan data\n",
        "full_data = pd.merge(user_dataset, event_dataset, how='cross')\n",
        "\n",
        "# Pisahkan data menjadi train dan test\n",
        "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encoding dan penggabungan data\n",
        "user_skills_split = user_dataset['Skills'].str.split(', ', expand=True)\n",
        "user_skills_one_hot = pd.get_dummies(user_skills_split, prefix='Skill')\n",
        "\n",
        "user_one_hot_train = pd.get_dummies(train_data[[ 'Skills', 'Location', 'Type of Organization']], prefix='User')\n",
        "event_one_hot_train = pd.get_dummies(train_data[['Category', 'Location', 'Qualifications']], prefix='Event')\n",
        "event_kualifikasi_one_hot_train = pd.get_dummies(train_data['Qualifications'].str.split(', ', expand=True), prefix='Qualifications')\n",
        "\n",
        "train_data_encode = pd.concat([user_one_hot_train, user_skills_one_hot, event_one_hot_train, event_kualifikasi_one_hot_train], axis=1)\n",
        "\n",
        "# Normalisasi data\n",
        "scaler = StandardScaler()\n",
        "train_data_normalize = scaler.fit_transform(train_data_encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCD999T_WEyd"
      },
      "outputs": [],
      "source": [
        "train_data_normalize = np.nan_to_num(train_data_normalize, nan=np.nanmean(train_data_normalize, axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4H06KBWNn4",
        "outputId": "793a47bb-0821-4018-d651-6964acb0d6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(np.isnan(train_data_normalize).any())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG_jvJo3VTF1"
      },
      "outputs": [],
      "source": [
        "# Hitung similarity matrix menggunakan cosine similarity\n",
        "similarity_matrix = cosine_similarity(train_data_normalize, train_data_normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-V7Xo8FWZsI"
      },
      "outputs": [],
      "source": [
        "print(f\"Cosine Similarity: {similarity_matrix[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmsNhHe3ZkU2"
      },
      "source": [
        "# This one using vectorize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwIcbYxmW0lV",
        "outputId": "8dfa27fa-d228-495e-eeb7-c01c4fc91e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2592    E_2593\n",
            "2683    E_2684\n",
            "2723    E_2724\n",
            "2735    E_2736\n",
            "2744    E_2745\n",
            "2531    E_2532\n",
            "2659    E_2660\n",
            "2668    E_2669\n",
            "2776    E_2777\n",
            "2399    E_2400\n",
            "Name: Event_id, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "user_df = pd.read_csv('users_dataset.csv')\n",
        "event_df = pd.read_csv('events_dataset.csv')\n",
        "\n",
        "# Preprocessing\n",
        "user_df['Skills'] = user_df['Skills'].apply(lambda x: ' '.join(x.lower().split(', ')) if pd.notnull(x) else '')\n",
        "event_df['Qualifications'] = event_df['Qualifications'].apply(lambda x: ' '.join(x.lower().split(', ')) if pd.notnull(x) else '')\n",
        "\n",
        "# Vectorize the skills and qualifications\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "user_matrix = vectorizer.fit_transform(user_df['Skills'])\n",
        "event_matrix = vectorizer.transform(event_df['Qualifications'])\n",
        "\n",
        "# Compute the cosine similarity\n",
        "cosine_sim = linear_kernel(user_matrix, event_matrix)\n",
        "\n",
        "# Function to get recommendations\n",
        "def get_recommendations(user_index, cosine_sim=cosine_sim):\n",
        "    # Get the pairwsie similarity scores of all events for that user\n",
        "    sim_scores = list(enumerate(cosine_sim[user_index]))\n",
        "\n",
        "    # Sort the events based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the scores of the 10 most similar events\n",
        "    sim_scores = sim_scores[0:10]\n",
        "\n",
        "    # Get the event indices\n",
        "    event_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 10 most similar events\n",
        "    return event_df['Event_id'].iloc[event_indices]\n",
        "\n",
        "# Test the system relation user 1 (index 0) to event\n",
        "print(get_recommendations(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This one using Tokenizer NLP\n"
      ],
      "metadata": {
        "id": "Jzx6isyunkAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the datasets\n",
        "user_df = pd.read_csv('users_dataset.csv')\n",
        "event_df = pd.read_csv('events_dataset.csv')\n",
        "\n",
        "# Preprocessing\n",
        "user_df['Skills'] = user_df['Skills'].apply(lambda x: ' '.join(x.lower().split(', ')) if pd.notnull(x) else '')\n",
        "event_df['Qualifications'] = event_df['Qualifications'].apply(lambda x: ' '.join(x.lower().split(', ')) if pd.notnull(x) else '')\n",
        "\n",
        "# Tokenize the skills and qualifications\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(pd.concat([user_df['Skills'], event_df['Qualifications']]))\n",
        "\n",
        "user_sequences = tokenizer.texts_to_sequences(user_df['Skills'])\n",
        "event_sequences = tokenizer.texts_to_sequences(event_df['Qualifications'])\n",
        "\n",
        "# Pad the sequences\n",
        "user_data = pad_sequences(user_sequences)\n",
        "event_data = pad_sequences(event_sequences)\n",
        "\n",
        "# Define the model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 50\n",
        "num_filters = 10\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=user_data.shape[1]),\n",
        "    Conv1D(num_filters, kernel_size, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(user_data, np.ones(len(user_data)), epochs=5, verbose=1)\n",
        "\n",
        "# Compute recommendations\n",
        "user_embeddings = model.get_layer(index=0).get_weights()[0]\n",
        "event_embeddings = model.get_layer(index=0).get_weights()[0]\n",
        "\n",
        "def recommend_events(user_id, num_recommendations=5):\n",
        "    user_embedding = user_embeddings[user_id]\n",
        "    similarities = np.dot(event_embeddings, user_embedding)\n",
        "    event_ids = np.argsort(-similarities)[:num_recommendations]\n",
        "    return event_df['Event_id'].iloc[event_ids]\n",
        "\n",
        "# Test the recommendation system\n",
        "# print(recommend_events(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkyfhL3kkK4D",
        "outputId": "6fa7d0e2-c73d-4257-efa7-a1519230deac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "88/88 [==============================] - 5s 36ms/step - loss: 0.1595\n",
            "Epoch 2/5\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 8.7756e-05\n",
            "Epoch 3/5\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 8.6267e-06\n",
            "Epoch 4/5\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 2.6305e-06\n",
            "Epoch 5/5\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 9.1196e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the recommendation system\n",
        "print(recommend_events(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsXv6nXkVQl",
        "outputId": "b52e6f9f-0360-4a28-ef79-a6a7e7efdb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1     E_2,Youth Development,Jakarta,>18,\"Mentoring, ...\n",
            "9     E_10,Youth Development,\"Maluku, Banda Neira\",1...\n",
            "29                                                 E_30\n",
            "73                                                 E_74\n",
            "49    E_50,Youth Development,Bandung,>20,\"Team build...\n",
            "Name: Event_id, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load your datasets\n",
        "events = pd.read_csv('events_dataset.csv')\n",
        "users = pd.read_csv('users_dataset.csv')\n",
        "\n",
        "# Preprocessing\n",
        "events['Qualifications'] = events['Qualifications'].apply(lambda x: ' '.join(x.lower().split(',')) if pd.notnull(x) else '')\n",
        "users['Skills'] = users['Skills'].apply(lambda x: ' '.join(x.lower().split(','))if pd.notnull(x) else '')\n",
        "\n",
        "# Vectorize the qualifications and skills\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "events_matrix = vectorizer.fit_transform(events['Qualifications'])\n",
        "users_matrix = vectorizer.transform(users['Skills'])\n",
        "\n",
        "# Compute the cosine similarity\n",
        "cosine_sim = cosine_similarity(users_matrix, events_matrix)\n",
        "\n",
        "# Convert the cosine similarity matrix to a DataFrame\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim, columns=events['Event_id'], index=users['Volunteer Name'])\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[len(cosine_sim_df.columns)]),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "target = users['Volunteer Name']\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Train the model\n",
        "model.fit(cosine_sim_df,target ,epochs=10)\n"
      ],
      "metadata": {
        "id": "Fksenmb2SA-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_r80-AkGh2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⛹"
      ],
      "metadata": {
        "id": "hWvLxj-_Giw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using One Hot and Tokenizer 🉐\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "paFumD-IGw2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Event dataset\n",
        "event_data = pd.read_csv(\"./events_dataset.csv\",usecols=['Event_id','Category','Location','Qualifications'])\n",
        "event_df = pd.DataFrame(event_data,)\n",
        "\n",
        "# Load User dataset\n",
        "user_data = pd.read_csv(\"./users_dataset.csv\",usecols=['Volunteer Name','Skills','Location','Type of Organization'])\n",
        "user_df = pd.DataFrame(user_data)\n",
        "\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000\n",
        "\n",
        "# Split event and user data into training and testing sets\n",
        "event_train, event_test = train_test_split(event_df, test_size=0.2, random_state=42)\n",
        "user_train, user_test = train_test_split(user_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer Train and Test Qualifications\n",
        "tokenizer_qualification = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer_qualification.fit_on_texts(event_train['Qualifications'])\n",
        "\n",
        "qualification_seq = tokenizer_qualification.texts_to_sequences(event_train['Qualifications'])\n",
        "qualification_pad = pad_sequences(qualification_seq, maxlen=max_length, padding=padding_type, truncating= trunc_type)\n",
        "\n",
        "qualification_seq_test = tokenizer_qualification.texts_to_sequences(event_test['Qualifications'])\n",
        "qualification_pad_test = pad_sequences(qualification_seq_test, maxlen=max_length, padding=padding_type, truncating= trunc_type)\n",
        "\n",
        "# Tokenizer Train and Test Skill\n",
        "tokenizer_skill = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer_skill.fit_on_texts(user_train['Skills'])\n",
        "\n",
        "skill_seq = tokenizer_skill.texts_to_sequences(user_train['Skills'])\n",
        "skill_pad = pad_sequences(skill_seq, maxlen=max_length, padding=padding_type, truncating= trunc_type)\n",
        "\n",
        "skill_seq_test = tokenizer_skill.texts_to_sequences(user_test['Skills'])\n",
        "skill_pad_test = pad_sequences(skill_seq_test, maxlen=max_length, padding=padding_type, truncating= trunc_type)\n",
        "\n",
        "# One hot encoding Event\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "event_cat_loc_org_encoded_train = encoder.fit_transform(event_train[['Category', 'Location']])\n",
        "event_cat_loc_org_encoded_test = encoder.transform(event_test[['Category', 'Location']])\n",
        "\n",
        "# One hot encoding user\n",
        "user_loc_org_encoded_train = encoder.fit_transform(user_train[['Location', 'Type of Organization']])\n",
        "user_loc_org_encoded_test = encoder.transform(user_test[['Location', 'Type of Organization']])\n",
        "\n",
        "# Build user model\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, name='user_embedding')  # output layer for user model\n",
        "])\n",
        "\n",
        "# Build event model\n",
        "event_NN = tf.keras.models.Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, name='event_embedding')  # output layer for event model\n",
        "])\n",
        "\n",
        "# Inputs for user and event\n",
        "input_user_skills = Input(shape=(max_length,), name='input_user_skills')\n",
        "input_user_loc_org = Input(shape=(user_loc_org_encoded_train.shape[1],), name='input_user_loc_org')\n",
        "input_event_qualifications = Input(shape=(max_length,), name='input_event_qualifications')\n",
        "input_event_cat_loc_org = Input(shape=(event_cat_loc_org_encoded_train.shape[1],), name='input_event_cat_loc_org')\n",
        "\n",
        "# Call user and event models\n",
        "vu_skills = user_NN(input_user_skills)\n",
        "vu_loc_org = Dense(128, activation='relu')(input_user_loc_org)\n",
        "vu = Concatenate()([vu_skills, vu_loc_org])\n",
        "\n",
        "vm_qualifications = event_NN(input_event_qualifications)\n",
        "vm_cat_loc_org = Dense(128, activation='relu')(input_event_cat_loc_org)\n",
        "vm = Concatenate()([vm_qualifications, vm_cat_loc_org])\n",
        "\n",
        "\n",
        "# Specify the inputs and outputs of the model\n",
        "model = tf.keras.Model([input_user_skills, input_user_loc_org, input_event_qualifications, input_event_cat_loc_org], [vu, vm])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9x7Hp1NGhmo",
        "outputId": "27304170-3ea1-4d89-d6de-fc1b00300548"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_user_skills (InputLa  [(None, 120)]                0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " input_user_loc_org (InputL  [(None, 97)]                 0         []                            \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " input_event_qualifications  [(None, 120)]                0         []                            \n",
            "  (InputLayer)                                                                                    \n",
            "                                                                                                  \n",
            " input_event_cat_loc_org (I  [(None, 23)]                 0         []                            \n",
            " nputLayer)                                                                                       \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)   (None, 64)                   531904    ['input_user_skills[0][0]']   \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 128)                  12544     ['input_user_loc_org[0][0]']  \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)   (None, 64)                   531904    ['input_event_qualifications[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 128)                  3072      ['input_event_cat_loc_org[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 192)                  0         ['sequential_4[0][0]',        \n",
            " )                                                                   'dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 192)                  0         ['sequential_5[0][0]',        \n",
            " )                                                                   'dense_11[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1079424 (4.12 MB)\n",
            "Trainable params: 1079424 (4.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# This one if there is already have interact user to event\n",
        "Jadi make dot.\n",
        "```\n",
        "## Inputs for user and event\n",
        "# input_user_skills = Input(shape=(max_length,), name='input_user_skills')\n",
        "# input_user_loc_org = Input(shape=(user_loc_org_encoded_train.shape[1],), name='input_user_loc_org')\n",
        "# input_event_qualifications = Input(shape=(max_length,), name='input_event_qualifications')\n",
        "# input_event_cat_loc_org = Input(shape=(event_cat_loc_org_encoded_train.shape[1],), name='input_event_cat_loc_org')\n",
        "\n",
        "\n",
        "## Call user and event models\n",
        "# vu_skills = user_NN(input_user_skills)\n",
        "# vu_loc_org = Dense(128, activation='relu')(input_user_loc_org)\n",
        "# vu = Concatenate()([vu_skills, vu_loc_org])\n",
        "\n",
        "# vm_qualifications = event_NN(input_event_qualifications)\n",
        "# vm_cat_loc_org = Dense(128, activation='relu')(input_event_cat_loc_org)\n",
        "# vm = Concatenate()([vm_qualifications, vm_cat_loc_org])\n",
        "\n",
        "# Compute the dot product of the two vectors vu and vm\n",
        "# output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# Specify the inputs and output of the model\n",
        "# model = tf.keras.Model([input_user_skills, input_user_loc_org, input_event_qualifications, input_event_cat_loc_org], output)\n",
        "# model.summary()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "f-98pbJJLA1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "oParBOFwIBG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}