{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argalusmp/CH2-PS_Recommendation-System/blob/faiqa/capstone_recsys_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-reQ11gbLB"
      },
      "source": [
        "Trying to complete the project, we build a simple matrix factorization model using our csv dataset of `Volunteering Datasets` heavily referencing the MovieLens 100k reccomendation system with TFRS. While the model is used to recommend movies for a given user, this altered set recommend volunteering events to the volunteer appliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA00wBE2Ntdm"
      },
      "source": [
        "#### Import TFRS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yzAaM85Z12D"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3oYt3R6Nr9l"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxQ1CZcO2wh"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-mxBYjdO5m7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Volunteers data\n",
        "volunteers = pd.read_csv('/content/drive/MyDrive/CH2-PS374 | Capstone Project/dataset_volunteers.csv')\n",
        "# events data\n",
        "events = pd.read_csv('/content/drive/MyDrive/CH2-PS374 | Capstone Project/dataset_events.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the data"
      ],
      "metadata": {
        "id": "t2JWjpJ-NYdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_volunteers = pd.DataFrame(volunteers)\n",
        "df_events = pd.DataFrame(events)"
      ],
      "metadata": {
        "id": "QN7XrobEOGrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data type\n",
        "df_volunteers = df_volunteers.astype(str)\n",
        "df_events = df_events.astype(str)"
      ],
      "metadata": {
        "id": "CiHx8Ps0NVnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_dict = pd.concat([df_volunteers, df_events], axis=0)"
      ],
      "metadata": {
        "id": "m6UReEeGS8lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_dict = both_dict.astype(str)"
      ],
      "metadata": {
        "id": "SwcXufxSTy7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_ds = tf.data.Dataset.from_tensor_slices(dict(both_dict))"
      ],
      "metadata": {
        "id": "ZIGHHzBiTlXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer_ds = tf.data.Dataset.from_tensor_slices(dict(df_volunteers))\n",
        "event_ds = tf.data.Dataset.from_tensor_slices(dict(df_events))"
      ],
      "metadata": {
        "id": "QPCsd_j-OcbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_both_features(element):\n",
        "    return {\n",
        "        \"Volunteer Name\": element[\"Volunteer Name\"],\n",
        "        \"Type of Organization\": element[\"Type of Organization\"],\n",
        "        \"Location\": element[\"Location\"],\n",
        "        \"Skill 1\": element[\"Skill 1\"],\n",
        "        \"Skill 2 (Additional)\": element[\"Skill 2 (Additional)\"],\n",
        "        \"Event_id\": element[\"Event_id\"],\n",
        "        \"Category\": element[\"Category\"],\n",
        "        \"Location\": element[\"Location\"],\n",
        "        \"Qualifications 1\": element[\"Qualifications 1\"],\n",
        "        \"Qualifications 2\": element[\"Qualifications 2\"]\n",
        "        # Include other features to keep\n",
        "    }"
      ],
      "metadata": {
        "id": "N5ppU9eTUMQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#both_ds = both_ds.map(lambda x: tf.py_function(np.array(map_both_features), inp=[x], Tout=[tf.strings]))"
      ],
      "metadata": {
        "id": "cpfw9SOgiS9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_ds = both_ds.map(lambda x: tf.py_function(map_both_features, [x], [tf.strings]))"
      ],
      "metadata": {
        "id": "dMCwbfEUU7tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_volunteer_features(element):\n",
        "    return {\n",
        "        \"Volunteer Name\": element[\"Volunteer Name\"],\n",
        "        \"Type of Organization\": element[\"Type of Organization\"],\n",
        "        # Include other features to keep\n",
        "    }\n",
        "\n",
        "def map_event_features(element):\n",
        "    return {\n",
        "        \"Event_id\": element[\"Event_id\"],\n",
        "        \"Category\": element[\"Category\"],\n",
        "        # Include other features to keep\n",
        "    }"
      ],
      "metadata": {
        "id": "UN24M_0PUmLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer_ds = volunteer_ds.map(map_volunteer_features)\n",
        "event_ds = event_ds.map(map_event_features)"
      ],
      "metadata": {
        "id": "M6DcmN7pUjvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_ds = both_ds.map(map_both_features)"
      ],
      "metadata": {
        "id": "NNSd4zYzpUVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_volunteers = np.unique(np.concatenate(list(volunteer_ds.batch(1000).map(lambda x: x[\"Volunteer Name\"]))))\n",
        "unique_events = np.unique(np.concatenate(list(both_ds.batch(1_000).map(lambda x: x[\"Event_id\"]))))"
      ],
      "metadata": {
        "id": "B5tMP4SPPjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly shuffle data and split between train and test.\n",
        "tf.random.set_seed(42)\n",
        "shuffled = both_ds.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(600)\n",
        "test = shuffled.skip(600).take(200)"
      ],
      "metadata": {
        "id": "-cHe_LVEQuLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’— trying another type of **retrieval** model ðŸ’—"
      ],
      "metadata": {
        "id": "BX6-jgTXCMYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VolunteeringModel(tfrs.Model):\n",
        "\n",
        "    def __init__(self, user_model, event_model):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Candidate model (item)\n",
        "        ### This is Keras preprocessing layers to first convert user ids to integers,\n",
        "        ### and then convert those to user embeddings via an Embedding layer.\n",
        "        ### We use the list of unique user ids we computed earlier as a vocabulary:\n",
        "        event_model = tf.keras.Sequential([\n",
        "                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                                        vocabulary=unique_events, mask_token=None),\n",
        "                                        tf.keras.layers.Embedding(len(unique_events) + 1, embedding_dimension)\n",
        "                                        ])\n",
        "        ### we pass the embedding layer into item model\n",
        "        self.event_model: tf.keras.Model = event_model\n",
        "\n",
        "        ### Query model (users)\n",
        "        user_model = tf.keras.Sequential([\n",
        "                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                                        vocabulary=unique_volunteers, mask_token=None),\n",
        "                                        # We add an additional embedding to account for unknown tokens.\n",
        "                                        tf.keras.layers.Embedding(len(unique_volunteers) + 1, embedding_dimension)\n",
        "                                        ])\n",
        "        self.user_model: tf.keras.Model = user_model\n",
        "\n",
        "        ### for retrieval model. we take top-k accuracy as metrics\n",
        "        metrics = tfrs.metrics.FactorizedTopK(candidates=event_ds.batch(128).map(lambda x: event_model(x[\"Category\"])))\n",
        "\n",
        "        # define the task, which is retrieval\n",
        "        task = tfrs.tasks.Retrieval(metrics=metrics)\n",
        "\n",
        "        self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        # We pick out the user features and pass them into the user model.\n",
        "        user_embeddings = self.user_model(features[\"Volunteer Name\"])\n",
        "        # And pick out the movie features and pass them into the movie model,\n",
        "        # getting embeddings back.\n",
        "        event_embeddings = self.event_model(features[\"Event_id\"])\n",
        "\n",
        "        # The task computes the loss and the metrics.\n",
        "        return self.task(user_embeddings, event_embeddings)"
      ],
      "metadata": {
        "id": "GxyolfZXB0VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Fitting and evaluating\n",
        "\n",
        "### we choose the dimensionality of the query and candicate representation.\n",
        "embedding_dimension = 32\n",
        "\n",
        "## we pass the model, which is the same model we created in the query and candidate tower, into the model\n",
        "event_model = tf.keras.Sequential([\n",
        "                                tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                                vocabulary=unique_events, mask_token=None),\n",
        "                                tf.keras.layers.Embedding(len(unique_events) + 1, embedding_dimension)\n",
        "                                ])\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "                                tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                                vocabulary=unique_volunteers, mask_token=None),\n",
        "                                # We add an additional embedding to account for unknown tokens.\n",
        "                                tf.keras.layers.Embedding(len(unique_volunteers) + 1, embedding_dimension)\n",
        "                                ])\n",
        "\n",
        "model = VolunteeringModel(user_model, event_model)\n",
        "\n",
        "# a smaller learning rate may make the model move slower and prone to overfitting, so we stick to 0.1\n",
        "# other optimizers, such as SGD and Adam\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01))\n",
        "\n",
        "cached_train = train.shuffle(10).batch(32).cache()\n",
        "cached_test = test.batch(32).cache()\n",
        "\n",
        "## fit the model with ten epochs\n",
        "model_hist = model.fit(cached_train, epochs=10)"
      ],
      "metadata": {
        "id": "Q1qqi7veC8RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_test = test.batch(32).cache()"
      ],
      "metadata": {
        "id": "3Ivdmu9Fxf6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "metadata": {
        "id": "Oh4yJ3D0DELc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "# recommends movies out of the entire movies dataset.\n",
        "index.index(event_ds.batch(16).map(model.event_model), event_ds)"
      ],
      "metadata": {
        "id": "v4hf6EPuDMCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recommendations version 1\n",
        "j = str(40)\n",
        "_, titles = index(tf.constant([j]))\n",
        "print(f\"Recommendations for user %s: {titles[0]}\" %(j))"
      ],
      "metadata": {
        "id": "btA9X3ja1Fx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some recommendations version 2\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(f\"Top 3 recommendations for user 42: {titles[0, :3]}\")"
      ],
      "metadata": {
        "id": "-4KThtgT1ALK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒž then with **ranking** model ðŸŒž"
      ],
      "metadata": {
        "id": "wtJS7WgoDmSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RankingModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        embedding_dimension = 32\n",
        "\n",
        "        # Compute embeddings for users.\n",
        "        self.user_embeddings = tf.keras.Sequential([\n",
        "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=unique_volunteers, mask_token=None),\n",
        "          tf.keras.layers.Embedding(len(unique_volunteers) + 1, embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        # Compute embeddings for movies.\n",
        "        self.event_embeddings = tf.keras.Sequential([\n",
        "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=unique_events, mask_token=None),\n",
        "          tf.keras.layers.Embedding(len(unique_events) + 1, embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        # Compute predictions.\n",
        "        self.ratings = tf.keras.Sequential([\n",
        "          # Learn multiple dense layers.\n",
        "          tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "          tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "          # Make rating predictions in the final layer.\n",
        "          tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        user_name, event_id = inputs\n",
        "\n",
        "        user_embeddings = self.user_embeddings(user_name)\n",
        "        event_embeddings = self.event_embeddings(event_id)\n",
        "\n",
        "        return self.ratings(tf.concat([user_embeddings, event_embeddings], axis=1))"
      ],
      "metadata": {
        "id": "BPzF9-_1DltK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VolunteeringModel(tfrs.models.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ranking_model: tf.keras.Model = RankingModel()\n",
        "        self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "          loss = tf.keras.losses.MeanSquaredError(),\n",
        "          metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        rating_predictions = self.ranking_model(\n",
        "            (features[\"Volunteer Name\"], features[\"Event_id\"]))\n",
        "\n",
        "        # The task computes the loss and the metrics.\n",
        "        return self.task(labels=features[\"Category\"], predictions=rating_predictions)"
      ],
      "metadata": {
        "id": "mN_kyZolE8_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VolunteeringModel()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.5))\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(32).cache()\n",
        "cached_test = test.batch(32).cache()\n",
        "\n",
        "model.fit(cached_train, epochs=100)\n",
        "\n",
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "metadata": {
        "id": "fU14e7UDFAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾ âš¾\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VQHVpN9JCZsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5dNbDZwOIHR"
      },
      "outputs": [],
      "source": [
        "class VolunteeringModel(tfrs.Model):\n",
        "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
        "  # these are still plain Keras Models.\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      volunteers_model: tf.keras.Model,\n",
        "      events_model: tf.keras.Model,\n",
        "      task: tfrs.tasks.Retrieval):\n",
        "    super().__init__()\n",
        "\n",
        "    # Set up user and movie representations.\n",
        "    self.volunteers_model = volunteers_model\n",
        "    self.events_model = events_model\n",
        "\n",
        "    # Set up a retrieval task.\n",
        "    self.task = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # Define how the loss is computed.\n",
        "\n",
        "    volunteers_embeddings = self.volunteers_model(features[\"Volunteer Name\"])\n",
        "    events_embeddings = self.events_model(features[\"Event_id\"])\n",
        "\n",
        "    return self.task(volunteers_embeddings, events_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwtgUCEOI8y"
      },
      "source": [
        "Define the two models and the retrieval task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvtnUN6aUY4U"
      },
      "outputs": [],
      "source": [
        "# Define user and movie models.\n",
        "volunteers_model = tf.keras.Sequential([\n",
        "    volunteers_set,\n",
        "    tf.keras.layers.Embedding(volunteers_set.vocab_size(), 64)\n",
        "])\n",
        "events_model = tf.keras.Sequential([\n",
        "    event_ids,\n",
        "    tf.keras.layers.Embedding(event_ids.vocab_size(), 64)\n",
        "])\n",
        "\n",
        "# Define your objectives.!!!!\n",
        "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
        "    mapped_event_ds\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMV0HpzmJGWk"
      },
      "source": [
        "\n",
        "### Fit the Model and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2tQDhqkOKf1"
      },
      "outputs": [],
      "source": [
        "# Create a retrieval model\n",
        "model = VolunteeringModel(volunteers_model, events_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for epochs\n",
        "model.fit(mapped_volunteer_ds.batch(128), epochs=3)\n",
        "\n",
        "# Use brute-force search to set up retrieval using the trained representations.\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.volunteers_model)\n",
        "index.index_from_dataset(\n",
        "    mapped_event_ds.batch(100).map(lambda Event_id: (Event_id, model.events_model(Event_id))))\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(f\"Top 3 recommendations for user 42: {titles[0, :3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dumps ðŸ§»"
      ],
      "metadata": {
        "id": "VUNfoJ49g-oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.get_dummies(df_volunteers, columns=['Skill 1', 'Skill 2 (Additional)', 'Location', 'Type of Organization'])\n",
        "event = pd.get_dummies(df_events, columns=['Qualifications 1', 'Qualifications 2', 'Location', 'Category'])"
      ],
      "metadata": {
        "id": "kKWN1ijg3O-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align data frame on both sets\n",
        "volunteer, event = volunteer.align(event, fill_value=0, axis=1)"
      ],
      "metadata": {
        "id": "hjbQMVuD5J3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type to tensor slices\n",
        "volunteer = tf.data.Dataset.from_tensor_slices(dict(volunteer))\n",
        "event = tf.data.Dataset.from_tensor_slices(dict(event))"
      ],
      "metadata": {
        "id": "75ZVcFPapXyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = volunteer.map(lambda x: {\n",
        "    \"Volunteer Name\": x[\"Volunteer Name\"],\n",
        "    \"Type of Organization\": x.get(\"Type of Organization\", \"Unknown\")\n",
        "})"
      ],
      "metadata": {
        "id": "cFrHJnL-8rrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in volunteer.take(5):  # Print the first 5 elements\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "Ceh4IvNH8vvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the basic features.\n",
        "volunteer = volunteers.map(lambda x: {\n",
        "    \"Volunteer Name\": x[\"Volunteer Name\"],\n",
        "    \"Type of Organization\": x[\"Type of Organization\"]\n",
        "})\n",
        "event = events.map(lambda x: x[\"Category\"])"
      ],
      "metadata": {
        "id": "49s1sLbokKPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒ¹ Just defining using volunteer's skill ðŸŒ¹"
      ],
      "metadata": {
        "id": "wz7Wqe_gobQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just using skill as parameter\n",
        "df_volunteers = volunteers[['Volunteer Name', 'Type of Organization']]\n",
        "df_volunteers = pd.DataFrame(df_volunteers, columns = ['Volunteer Name', 'Type of Organization'])\n",
        "\n",
        "df_events = events[['Event_id', 'Category']]\n",
        "df_events = pd.DataFrame(df_events)"
      ],
      "metadata": {
        "id": "tepSMfS9lWx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_volunteers.info())\n",
        "print(df_events.info())"
      ],
      "metadata": {
        "id": "dfBRKDKO7CSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the pandas DataFrames to tensors\n",
        "volunteer_ds = tf.data.Dataset.from_tensor_slices(dict(df_volunteers))\n",
        "event_ds = tf.data.Dataset.from_tensor_slices(dict(df_events))"
      ],
      "metadata": {
        "id": "0lrlg4yHi1r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map rows to dictionary\n",
        "volunteer = volunteer_ds.map(lambda x:{\n",
        "    \"Volunteer Name\": x[\"Volunteer Name\"],\n",
        "    \"Type of Organization\": x[\"Type of Organization\"]\n",
        "})\n",
        "\n",
        "event = event_ds.map(lambda x: x[\"Category\"])\n",
        "    #\"Event_id\": x[\"Event_id\"],"
      ],
      "metadata": {
        "id": "1pL0rk9Pjdzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use these later for the embeddings\n",
        "usernames = volunteer.map(lambda x: x['Volunteer Name'])\n",
        "unique_users = np.unique(np.concatenate(list(usernames.batch(1000))))\n",
        "unique_events = np.unique(np.concatenate(list(event.batch(1000))))"
      ],
      "metadata": {
        "id": "iFDKTve4kWBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the volunteer dataset"
      ],
      "metadata": {
        "id": "6pzJbFc4oV7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "vshuffled = volunteer.shuffle(len(df_volunteers), seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "vtrain = vshuffled.take(int(len(df_volunteers)*0.8))\n",
        "vtest = vshuffled.skip(int(len(df_volunteers)*0.8)).take(int(len(df_volunteers)*0.2))"
      ],
      "metadata": {
        "id": "Bix8ZKG-m4_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "eshuffled = event.shuffle(len(df_events), seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "etrain = vshuffled.take(int(len(df_events)*0.8))\n",
        "etest = vshuffled.skip(int(len(df_events)*0.8)).take(int(len(df_events)*0.2))"
      ],
      "metadata": {
        "id": "RbD_KZ1ZorIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "BLul48UnpCju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VolunteerRetreival(tfrs.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dims = 32\n",
        "        self.user_model =  tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(\n",
        "                vocabulary= unique_users, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_users)+1, embedding_dims)\n",
        "        ])\n",
        "\n",
        "        self.event_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(\n",
        "                vocabulary=unique_events, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_events)+1, embedding_dims)\n",
        "        ])\n",
        "\n",
        "        self.task = tfrs.tasks.Retrieval(\n",
        "                        metrics=tfrs.metrics.FactorizedTopK(\n",
        "                        candidates=event.batch(128).cache().map(self.event_model)\n",
        "                        ))\n",
        "\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features['Volunteer Name'])\n",
        "        event_embeddings = self.event_model(features['Category'])\n",
        "        return self.task(user_embeddings, event_embeddings)"
      ],
      "metadata": {
        "id": "Dh5fZLApo94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VolunteerRetreival()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))"
      ],
      "metadata": {
        "id": "uLDjh_tkqhe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = 2\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "                vtrain,\n",
        "                validation_data= vtest,\n",
        "                validation_freq=freq,\n",
        "                epochs = epochs,\n",
        "                verbose = 0)"
      ],
      "metadata": {
        "id": "tpepEyDBqmn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping function for the volunteer dataset\n",
        "def map_volunteer_features(element):\n",
        "    return {\n",
        "        \"Volunteer Name\": element[\"Volunteer Name\"],\n",
        "        \"Type of Organization\": element[\"Type of Organization\"],\n",
        "        \"Location\": element[\"Location\"],\n",
        "        \"Skill 1\": element[\"Skill 1\"],\n",
        "        \"Skill 2 (Additional)\": element[\"Skill 2 (Additional)\"]\n",
        "        # Include other features to keep\n",
        "    }\n",
        "\n",
        "# Mapping function for the event dataset\n",
        "def map_event_features(element):\n",
        "    return {\n",
        "        \"Event_id\": element[\"Event_id\"],\n",
        "        \"Category\": element[\"Category\"],\n",
        "        \"Location\": element[\"Location\"],\n",
        "        \"Qualifications 1\": element[\"Qualifications 1\"],\n",
        "        \"Qualifications 2\": element[\"Qualifications 2\"]\n",
        "        # Include other features to keep\n",
        "    }\n",
        "\n",
        "# Apply the mapping functions to the datasets\n",
        "volunteer_ds = volunteer_ds.map(map_volunteer_features)\n",
        "event_ds = event_ds.map(map_event_features)\n",
        "\n",
        "# Print some elements to verify the mapping\n",
        "for element in volunteer_ds.take(5):\n",
        "    print(element)\n",
        "\n",
        "for element in event_ds.take(5):\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "76SfxUPXBpXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}